<!doctype html>
<html lang="zh-CN">
<head>

    <meta charset="utf-8">
    <meta name="generator" content="Hugo 0.58.3" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>神经网络调试清单 | 马氏大院</title>
    <meta property="og:title" content="神经网络调试清单 - 马氏大院">
    <meta property="og:type" content="article">
        
    <meta property="article:published_time" content="2019-10-16T19:18:58&#43;08:00">
        
        
    <meta property="article:modified_time" content="2019-10-16T19:18:58&#43;08:00">
        
    <meta name="Keywords" content="">
    <meta name="description" content="神经网络调试清单">
        
    <meta name="author" content="">
    <meta property="og:url" content="https://offduty.github.io/posts/20190116/">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

    <link rel="stylesheet" href="/css/normalize.css">
    
        <link rel="stylesheet" href="/css/prism.css">
    
    <link rel="stylesheet" href="/css/style.css">
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

    


    
    
</head>

<body>
<header id="header" class="clearfix">
    <div class="container">
        <div class="col-group">
            <div class="site-name ">
                
                    <a id="logo" href="https://offduty.github.io">
                        马氏大院
                    </a>
                
                
            </div>
            <div>
                <nav id="nav-menu" class="clearfix">
                    <a class="" href="https://offduty.github.io">博客</a>
                    
                    <a  href="https://offduty.github.io/archives/" title="归档">归档</a>
                    
                    <a  href="https://offduty.github.io/about/" title="关于">关于</a>
                    
                </nav>
            </div>
        </div>
    </div>
</header>


<div id="body">
        
        
    <div class="container">
        <div class="col-group">

            <div class="col-8" id="main">
                <div class="res-cons">
                    <article class="post">
                        <header>
                            <h1 class="post-title">神经网络调试清单</h1>
                        </header>
                        <date class="post-meta meta-date">
                            2019年10月16日
                        </date>
                        
                        
                        
                        <div class="post-content">
                            

<p>你可以采取实质性步骤去识别和修复机器学习模型的训练、泛化和优化问题。</p>

<p>众所周知，由于机器学习 bugs 的捕捉代价很高，导致机器学习代码调试起来很困难。即使是简单的前馈神经网络，你通常也不得不围绕可能导致机器学习代码出现潜在错误的网络架构，权重初始化和网络优化等几方面去作决策。
正如 Chase Roberts 在“如何对机器学习代码进行单元测试”的文章中所写的那样，他的挫败感源于常见的陷阱，比如：
1. 代码没有崩溃， 也没有抛出异常，甚至也没有出现运行速度的变慢。
2. 网络仍在训练，loss 值也在减少。
3. 数小时候后，这些值变得收敛，但是结果很差。</p>

<p>那我们应该怎么办？</p>

<p>本文将提供一个框架帮助你调试你的神经网络。
1. 从简单处开始
2. 验证损失
3. 检查中间层的输出结果和连接关系
4. 诊断参数
5. 追踪你的工作
请随意跳到特定的部分或通读下面的内容。请注意:本文不涵盖数据预处理或特定的模型算法选择。关于这些主题，网上有很多的资源(例如 <a href="https://hackernoon.com/choosing-the-right-machine-learning-algorithm-68126944ce1f">choosing the right machine learning algorithm</a>)。</p>

<h2 id="从简单处开始">从简单处开始</h2>

<p>与简单网络相比，一个具有正则化和学习率调度程序的复杂体系结构的神经网络将更难以调试。对于第一点，我们有一点欺骗，因为它与调试您已经建立的网络并没有真正的联系，但这仍然是一个重要的建议！
从以下简单点开始：
- 首先构建一个简单的模型
- 在单个数据点上训练模型</p>

<p><strong>构建一个简单的模型</strong>
首先，请构建一个具有单个隐藏层的小型网络，并验证一切正常。然后逐渐增加模型的复杂度，同时检查模型结构的各个方面(如附加层、参数等)，然后再继续。</p>

<p><strong>在单个数据点上进行训练</strong>
为了快速检查模型的完整性，您可以使用一个或两个训练数据点来确认您的模型是否过拟合。神经网络应该立即出现过拟合，训练精度为100%，验证精度与模型的随机猜测相当。如果你的模型在这些数据点上没有出现过拟合，那么可能模型太小或存在 bug。</p>

<p>即使您已验证模型可以正常工作，也请尝试训练一个（或几个）epochs，然后再进行操作。</p>

<h2 id="验证损失">验证损失</h2>

<p>模型损失是评估模型性能的主要方法，也是模型评估的重要参数。因此你需要确保：
- 损失适合当前任务（比如分类交叉熵损失用于多分类问题，或使focal loss 用于类别不平衡问题）
- 确保损失函数的测量是在正确的尺度上进行的。 如果你在网络中使用的损失类型不止一种，例如MSE，adversarial，L1，feature loss，请确保将所有损失正确地缩放为相同数量级。
初始损失值也很重要。如果你的模型一开始是随机猜测的，检查一下初始损失是否接近你的预期损失。在 Coursera 课程 <a href="http://cs231n.github.io/neural-networks-3/#baby">Stanford CS23</a>中，Andrej Karpathy 给出了如下建议：</p>

<blockquote>
<p>寻找性能可能（at chance performance）正确的损失。使用小参数初始化时，请确保获得预期的损失。最好先单独检查数据丢失情况（将正则化强度设置为零）。 例如，对于CIFAR-10，由于具有Softmax分类器，我们预计初始损失为2.302，这是因为我们期望每个类别的弥散概率（diffuse probability）为0.1（因为有10个类别），而Softmax损失是该类别的负对数概率 正确的类，这样：$-ln（0.1）= 2.302$。</p>
</blockquote>

<p>对于一个二分类示例，您只需对每个类进行类似的计算。比如你的数据中 20% 的是 0 类，80% 的是 1 类，你的预期初始损失是$0.2ln(0.5)-0.8ln(0.5) = 0.693147$。如果你的初始损失比1大得多，这可能意味着你的神经网络的权重没有得到合适的平衡(即你的初始化很差)或者你的数据没有被标准化处理。</p>

<h2 id="检查中间输出和连接关系">检查中间输出和连接关系</h2>

<p>要调试神经网络，通常需要了解神经网络内部的动态以及各个中间层所起的作用以及这些层之间是如何连接的。你可能会遇到如下错误：
- 错误的梯度更新表达式
- 没有应用权重更新
- 梯度消失或爆炸
如果您的梯度值为零，这可能意味着优化器中的学习率太小，或者您遇到了上面的错误#1，梯度更新的表达式不正确。除了查看梯度更新的绝对值之外，还要确保监视每个层匹配的激活程度、权重和更新。比如，参数（权重和偏差）更新的幅度应该为 0.001.有一种现象叫做渐灭的ReLU（Dying ReLU）或梯度消失问题，ReLU神经元在学习了一个大的负偏置项后会输出零。这些神经元将再也不会在任何数据点上被激活。
你可以通过使用数值方法近似梯度的梯度检查方法来检查这些错误。如果它接近计算的梯度，那么表明反向传播是正确的。要实现梯度检查，可以查看 CS231 以及 Andrew Ng <a href="https://www.youtube.com/watch?v=P6EtCVrvYPU">具体课程</a>相关主题资源。
关于可视化神经网络，Faizan Shaikh给出了三种主要方法:
- 初级方法：简单的方法向我们展示了一个训练模型的整体结构。这些方法包括打印出神经网络各个层的形状或滤波器以及每个层的参数。
- 激活基本方法：在这些方法中，我们破译单个神经元或一组神经元的激活情况，以直观地了解它们在做什么
- 基于梯度的方法：这些方法倾向于在训练模型时操作由前向传播和反向传播形成的梯度(包括显著性映射和类激活映射)。</p>

<p>有许多有用的工具可用于可视化各层的激活和连接，例如ConX和Tensorboard。
&gt; 使用图像数据？ Erik Rippel在<a href="https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59">Visualizing parts of Convolutional Neural Networks using Keras and Cats</a>”一文中给出了详细的说明。</p>

<h2 id="4-诊断参数">4 诊断参数</h2>

<p>神经网络有大量的参数相互作用，使得优化变得困难。请注意，这是一个活跃的研究领域，所以下面的建议只是简单的出发点。</p>

<ul>
<li>Batch size 批处理大小（技术上称为迷你批处理）—您一方面希望批处理大小足够大以对梯度的误差进行准确估计，另一方面，有希望它足够小，小到 mini-batch随机梯度下降（SGD）能够调整网络。小批量生产将导致学习过程迅速收敛，但会损失训练过程中的噪音，并可能导致优化困难。论文《On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima’》给出了解释：
在实践中已经观察到，当使用较大批次时，模型的质量会降低，这是通过模型的泛化能力来衡量的。我们研究了在大批量情况下泛化下降的原因，并提供了数值证据来支持这样的观点:大批量方法倾向于收敛于训练和测试函数的急剧极小化，众所周知，急剧极小化导致较差的泛化。</li>
<li>Learning rate：过低的学习率将导致收敛速度变慢或陷入局部极小值的风险，而过大的学习率将导致优化发散，因为您有可能跳过损失函数中更深、更窄的部分。考虑结合学习速率，以随着训练的进行降低学习率。CS231n 课程中有很大的一节来介绍通过使用不同的技术实现退火学习率（annealing learning rates）。相反，小批量方法始终收敛于平坦的极小值，并且我们的实验支持一种普遍认为的观点，这是由于梯度估计中的固有噪声所致。</li>
</ul>

<blockquote>
<p>机器学习框架，比如 Keras，Tensorflow，PyTorch，MXNet 现在都有关于使用学习速率调度/衰减程序的文档或示例：</p>
</blockquote>

<p>Keras：<a href="https://keras.io/callbacks/#learningratescheduler">https://keras.io/callbacks/#learningratescheduler</a>
Tensorflow：<a href="https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay">https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay</a>
PyTorch：<a href="https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html">https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html</a>
MXNet： <a href="https://mxnet.incubator.apache.org/versions/master/tutorials/gluon/learning_rate_schedules.html">https://mxnet.incubator.apache.org/versions/master/tutorials/gluon/learning_rate_schedules.html</a></p>

<ul>
<li>梯度修剪 &ndash; 在反向传播期间，将以最大值或最大范数修剪参数的梯度。这对于解决您在上面的步骤3中可能遇到的任何爆炸梯度问题很有用。</li>
<li>Batch normalization：批处理归一化用于对各层的输入进行归一化处理，以解决内部协变量移位问题。如果同时使用Dropout和Batch Norma，请确保阅读以下关于Dropout的要点。
本文来自于<a href="https://towardsdatascience.com/pitfalls-of-batch-norm-in-tensorflow-and-sanity-checks-for-training-networks-e86c207548c8">Dishank Bansal ‘Pitfalls of Batch Norm in TensorFlow and Sanity Checks for Training Networks</a>，对于处理batch normalization的常见错误是一个很好的资源。</li>
</ul>

<p>随机梯度下降（SGD）- 有几种不同类型的SGD，它们使用动量（momentum），自适应学习率和Nesterov更新，但在训练性能和泛化方面都没有明显的赢家（请参阅Sebastian Ruder出色的“<a href="http://ruder.io/optimizing-gradient-descent/">梯度下降优化算法概述</a>”，以及 有趣的实验“ <a href="https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/">SGD&gt; Adam？</a>”）,建议的起点是Adam或具有Nesterov动量的纯SGD。
- Regularization &ndash; 正则化对于构建可泛化模型至关重要，因为它增加了对模型复杂性或极端参数值的惩罚.它显著的降低了模型的方差，而没有实质性增加偏差。如CS231n课程所述：
&gt; 损失函数通常是数据损失和正则化损失的总和（例如权重的L2损失）。需要注意的一个危险是，正则化损失可能超过数据损失，在这种情况下，梯度将主要来自正则化项(它通常有一个简单得多的梯度表达式)。这可能会掩盖数据损失梯度的实现错误。</p>

<p>要对此进行审核，您应该关闭正则化并独立检查数据丢失梯度。</p>

<ul>
<li>Dropout - drouout 是另一种使网络规范化以防止过度拟合的技术。 Drouout是通过在训练期间，以某个概率p（超参数）或将其设置为零来仅仅保持一个神经元的活跃。 结果，网络必须在每个训练批次中使用不同的参数子集，这样可以减少特定参数的主导其他参数的改变。</li>
<li>这里需要注意的是:如果您同时使用dropout和batch normalization (batch norm)，那么要注意这些操作的顺序，甚至要同时使用它们。这仍然是一个活跃的研究领域，但你可以看到最新的讨论</li>
</ul>

<blockquote>
<p>Stackoverflow 用户 MilominderBinder：
Dropout的意思是完全阻断来自某些神经元的信息，以确保神经元不相互适应。因此，批处理规范化必须在dropout之后进行，否则您将通过规范化统计信息传递信息</p>
</blockquote>

<p>来自 arXiv：Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift (Xiang Li, Shuo Chen, Xiaolin Hu, Jian Yang)从理论上讲，我们发现当我们将网络的状态从训练转移到测试时，Dropout会改变特定神经元的方差。 但是，BN将在测试阶段保持其统计差异，该差异是从整个学习过程中累积的。 当在BN之前应用Dropout时，该方差的不一致（我们将该方案称为“方差平移”）会导致推理中的不稳定数值行为，最终导致更多错误的预测。</p>

<h2 id="5-追踪你的工作">5 追踪你的工作</h2>

<p>你很容易忽视实验文档的的重要性，直到你忘记你使用的学习率或类权重值。通过更好的跟踪，您可以轻松地查看和重现以前的实验，以减少重复的工作（也就是遇到相同的错误）。</p>

<p>然而，手工记录信息是很困难的，并且在多个实验中都很难扩展。诸如<a href="http://bit.ly/2J6dxWA">Comet.ml</a>之类的工具可以帮助自动跟踪数据集，代码更改，实验历史和生产模型（其中包括有关模型的关键信息，例如超参数，模型性能指标和环境详细信息）。</p>

<p>您的神经网络对数据、参数甚至包版本的细微变化都非常敏感，这会导致模型性能的下降.追踪你的工作是开始标准化环境和建模工作流程的第一步。</p>

<h2 id="快速回顾">快速回顾</h2>

<p>我们希望这篇文章为调试你的神经网络提供了一个坚实的起点。本文重点，总结起来就是：
1. 从简单开始 &ndash; 首先建立一个更简单的模型，然后通过训练几个数据点进行测试
2. 确认损失 &ndash; 检查您使用的损失是否正确，并查看您的初始损失
3. 检查中间层的输出和连接关系 &ndash; 使用梯度检查和可视化去检查您的层是否正确连接和梯度如预期更新
4. 诊断参数 &ndash; 从SGD到学习率，确定正确的组合（或找出错误的组合）
5. 跟踪您的工作 &ndash; 作为基线，跟踪您的实验过程和关键建模组件。</p>

<p><a href="https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21">https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21</a></p>

                        </div>

                        
<div class="post-archive">
    <ul class="post-copyright">
        <li><strong>原文作者：</strong><a rel="author" href="https://offduty.github.io">马氏大院</a></li>
        <li style="word-break:break-all"><strong>原文链接：</strong><a href="https://offduty.github.io/posts/20190116/">https://offduty.github.io/posts/20190116/</a></li>
        <li><strong>版权声明：</strong>本作品采用<a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">知识共享署名-非商业性使用-禁止演绎 4.0 国际许可协议</a>进行许可，非商业转载请注明出处（作者，原文链接），商业转载请联系作者获得授权。</li>
    </ul>
</div>
<br/>



                        


                        <div class="post-meta meta-tags">
                            
                            <ul class="clearfix">
                                
                                <li><a href="https://offduty.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a></li>
                                
                                <li><a href="https://offduty.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">神经网络</a></li>
                                
                            </ul>
                            
                        </div>
                    </article>
                    
    

    
    
                </div>
            </div>
            <div id="secondary">
    <section class="widget">
        <form id="search" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" _lpchecked="1">
      
      <input type="text" name="q" maxlength="20" placeholder="Search">
      <input type="hidden" name="sitesearch" value="https://offduty.github.io">
      <button type="submit" class="submit icon-search"></button>
</form>
    </section>
    
    <section class="widget">
        <h3 class="widget-title">最近文章</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://offduty.github.io/posts/%E9%80%83%E9%80%B8%E7%9A%84%E7%BE%8E%E6%B4%B2%E7%8B%AE/" title="逃逸的美洲狮">逃逸的美洲狮</a>
    </li>
    
    <li>
        <a href="https://offduty.github.io/posts/%E8%8C%B6%E9%A6%86/" title="茶馆">茶馆</a>
    </li>
    
    <li>
        <a href="https://offduty.github.io/posts/20190116/" title="神经网络调试清单">神经网络调试清单</a>
    </li>
    
</ul>
    </section>

    

    <section class="widget">
        <h3 class="widget-title">分类</h3>
<ul class="widget-list">
    
</ul>
    </section>

    <section class="widget">
        <h3 class="widget-title">标签</h3>
<div class="tagcloud">
    
    <a href="https://offduty.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
    
    <a href="https://offduty.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a>
    
    <a href="https://offduty.github.io/tags/%E8%87%AA%E6%88%91%E4%BB%8B%E7%BB%8D/">自我介绍</a>
    
</div>
    </section>

    

    <section class="widget">
        <h3 class="widget-title">其它</h3>
        <ul class="widget-list">
            <li><a href="https://offduty.github.io/index.xml">文章 RSS</a></li>
        </ul>
    </section>
</div>
        </div>
    </div>
</div>
<footer id="footer">
    <div class="container">
        &copy; 2019 <a href="https://offduty.github.io">马氏大院 By </a>.
        Powered by <a rel="nofollow noreferer noopener" href="https://gohugo.io" target="_blank">Hugo</a>.
        <a href="https://www.flysnow.org/" target="_blank">Theme</a> based on <a href="https://github.com/rujews/maupassant-hugo" target="_blank">maupassant</a>.
        
    </div>
</footer>


    <script src="//cdnjs.cloudflare.com/ajax/libs/raphael/2.2.8/raphael.min.js" crossorigin="anonymous"></script>
        <script src="//cdnjs.cloudflare.com/ajax/libs/flowchart/1.12.2/flowchart.min.js" crossorigin="anonymous"></script>
        <script>(function () {
                if (!window.flowchart) return;
                const blocks = document.querySelectorAll('pre code.language-flowchart, pre code.language-flow');
                for (let i = 0; i < blocks.length; i++) {
                    const block = blocks[i];
                    const rootElement = block.parentNode;
                    const container = document.createElement('div');
                    const id = `js-flowchart-diagrams-${i}`;
                    container.id = id;
                    container.className = 'align-center';
                    container.setAttribute("style", "overFlow-x:auto");
                    rootElement.parentNode.replaceChild(container, rootElement);
                    const diagram = flowchart.parse(block.childNodes[0].nodeValue);
                    diagram.drawSVG(id, window.flowchartDiagramsOptions ? window.flowchartDiagramsOptions : {});
                }
            })();
        </script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/webfont/1.6.28/webfontloader.js" crossorigin="anonymous"></script>
        <script src="//cdnjs.cloudflare.com/ajax/libs/snap.svg/0.5.1/snap.svg-min.js" crossorigin="anonymous"></script>
        <script src="//cdnjs.cloudflare.com/ajax/libs/underscore.js/1.9.1/underscore-min.js" crossorigin="anonymous"></script>
        <script src="//cdnjs.cloudflare.com/ajax/libs/js-sequence-diagrams/1.0.6/sequence-diagram-min.js" crossorigin="anonymous"></script>
        <script>(function () {
            if (!window.Diagram) return;
            const blocks = document.querySelectorAll('pre code.language-sequence');
            for (let i = 0; i < blocks.length; i++) {
                const block = blocks[i];
                
                const rootElement = block.parentNode;
                const container = document.createElement('div');
                const id = `js-sequence-diag-${i}`;
                container.id = id;
                container.className = 'align-center';
                container.setAttribute("style", "overFlow-x:auto");
                rootElement.parentNode.replaceChild(container, rootElement);

                const diagram = Diagram.parse(block.childNodes[0].nodeValue);
                diagram.drawSVG(id, window.sequenceDiagramsOptions
                    ? window.sequenceDiagramsOptions
                    : { theme: 'simple' });
            }
        })();
        </script><script type="text/javascript">
        
        (function () {
            $("pre code").parent().addClass("line-numbers")
        }());

        window.MathJax = {
            tex2jax: {
                inlineMath: [['$', '$']],
                processEscapes: true
                }
            };
    </script>
    <script type="text/javascript" src="/js/prism.js" async="true"></script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>


<a id="rocket" href="#top"></a>
<script type="text/javascript" src="/js/totop.js?v=0.0.0" async=""></script>






</body>
</html>
